%DO NOT MESS AROUND WITH THE CODE ON THIS PAGE UNLESS YOU %REALLY KNOW WHAT YOU ARE DOING
 \chapter{Functions of random variables }

\section{Polynomial model }
\noindent \textbf{Task:} The signal model is given by
$$ Y_i = a_0 + a_1 x_i + a_2 {x_i}^2 + ...+ a_p {x_i}^p + Z_i  $$
\noindent Let the observation ($x_i, y_i$) : $i = 1,2,...,N$ be given. Derive the LS-Estimate for the unknown parameters $a_i$ : $i = 0,1,..., p$ and the estimate for the variance of the noise. How large has the number $N$ of observations to be at least? Express with regard to a MATLAB implementation the problem in vector notation.\\
\noindent \textbf{Solution:}\\
\noindent We have the signal model equation to be,
$$ Y_i = a_0 + a_1 x_i + a_2 {x_i}^2 + ...+ a_p {x_i}^p + Z_i  $$
\noindent The signal $Y_i$ is a random variable deterministic with noise. $p$ is the degree of the polynomials. In-order to determine this signal, we need to find its coefficients i.e., $a_0, a_1,...,a_p.$\\
\noindent Let us assume that the noise $Z_i$ is normally distributed and independent, i.e. $Z_i \sim N(o, {\sigma_Z}^2) $ with mean as 0 and variance, ${\sigma_Z}^2.$ The noise in a signal is given as,
$$Z_i  = Y_i -( a_0 + a_1 x_i + a_2 {x_i}^2 + ...+ a_p {x_i}^p)  $$
\noindent Therefore, our signal $Z_i \sim N( a_0 + a_1 x_i + a_2 {x_i}^2 + ...+ a_p {x_i}^p, {\sigma_Z}^2) $ has a different mean and hence it is not identically distributed. Hence, there is a change in the distribution. Density function for this case is, 
$$ fY_i(y_i) = \frac{1}{\sqrt{2\pi{\sigma_Z}^2}} \text{exp}\Big\{ \frac{-1}{2{\sigma_Z}^2} (Y_i -( a_0 + a_1 x_i + a_2 {x_i}^2 + ...+ a_p {x_i}^p) ) \Big\}     $$
\noindent Since ($x_i, y_i$) : $i = 1,2,...,N$, the product of all the terms gives us the density function. i.e.,
$$ f{Y_1}...{Y_N}({y_1}...{y_N}) = {(2\pi{\sigma_Z}^2)}^{-N/2} \text{exp}\Big\{ \frac{-1}{2{\sigma_Z}^2}\sum_{i=1}^{N} (Y_i -( a_0 + a_1 x_i + a_2 {x_i}^2 + ...+ a_p {x_i}^p) ) \Big\}  $$
\noindent We have to maximize the likelihood of its estimation to get the coefficient for the polynomial. We need to adjust the coefficient in such that the density function is maximum. Since $x_1, y_i$ are known, we maximise with respect to vector $a$  and ${\sigma_Z}^2$ to get the estimation.\\
\noindent Our maximizer estimator is,
$$ (\underline{\hat{a}}, {\sigma_Z}^2) = \text{argmax}(a, {\sigma_Z}^2)   $$
\noindent If we want to maximize the function, it is the same as minimizing the polynomial. Thus we define,
$$ q(a_0,..a_p) = \sum_{i=1}^{N} {(Y_i -( a_0 + a_1 x_i + a_2 {x_i}^2 + ...+ a_p {x_i}^p) }^2 $$
$$ q(a_0,..a_p) = \sum_{i=1}^{N} {Z_i }^2 $$
\noindent This shows that we have to minimize the noise. We know,\\
$$ Z_i = Y_i -( a_0 + a_1 x_i + a_2 {x_i}^2 + ...+ a_p {x_i}^p) $$
\noindent The above equation when taken the individual terms for, $i = 1,2,...,N$ can be expressed in vector form like,
\[
\begin{bmatrix}
z_1\\
z_2\\
.\\
.\\
.\\
z_n\\
\end{bmatrix}
=
\begin{bmatrix}
y_1\\
y_2\\
.\\
.\\
.\\
y_n\\
\end{bmatrix}
-
\begin{bmatrix}
1 & x_1 & x_1^2 & . & . & . & x_1^p \\ 
1 & x_2 & x_2^2 & . & . & . & x_2^p \\
. & . & . & . & . & . & . \\
. & . & . & . & . & . & . \\
. & . & . & . & . & . & . \\
1 & x_n & x_n^2 & . & . & . & x_n^p \\
\end{bmatrix}
\begin{bmatrix}
a_0\\
a_1\\
.\\
.\\
.\\
a_p\\
\end{bmatrix}
\]
\noindent In matrix domain it is given as,
$$ \underline{z} = \underline{y} - \underline{\underline{H}} \cdot \underline{a}$$
\noindent where $\underline{z}, \underline{a}$ and $\underline{y}$ are vectors and $\underline{\underline{H}}$ is a matrix of signal $x_i.$\\
\noindent If the number of independent equations are greater than the number of unknowns, we can determine the unknowns. Hence, the number of the observations should be greater than the number of the parameters that are to be found.
$$n \geq p+1$$
\noindent The equation $q(a)$ is define as to be equal to,
$$ q(a) = \sum_{i=0}^N(y_i - (a_0 + a_1x_1 + a_2x_1^2 + ...+a_px_i^p))=\sum_{i=1}^Nz_i^2 $$
\noindent where $  q(a) = q(a_0,..a_p) $
\noindent To find the sum, we have the property,
$$ \sum_{i=1}^Nz_i^2 = \underline{Z}^T \cdot \underline{Z} = \begin{bmatrix}
Z_1
.
.
.
Z_N
\end{bmatrix}   \begin{bmatrix}
Z_1\\
.\\
.\\
.\\
Z_N\\
\end{bmatrix}     $$
\noindent Therefore we get,
$$  q(a) = \underline{Z}^T \cdot \underline{Z}   $$
\noindent After substituting the value for $Z$ we get,
$$q(\underline{a}) = (\underline{y} - \underline{\underline{H}}\cdot \underline{a})^T(\underline{y} - \underline{\underline{H}}\cdot \underline{a})$$
\noindent In-order to minimise, we need to find the unknown parameters in $a$ that satisfy,
$$ \underline{\hat{a}} = \text{arg min}\; q(\underline{a})$$
\noindent Partially if we differentiate the equation with respect to $a_i$ and equate them to 0, we get the minimum. Now,
$$\frac{\partial q(a)}{\partial a_i} = \frac{ \partial}{ \partial a_i}(\underline{y} - \underline{\underline{H}} \cdot \underline{a})^T(\underline{y} - \underline{\underline{H}} \cdot \underline{a}) \quad \text{where,} \quad i=0,1,2....p$$
$$ = \frac{\partial}{\partial a_i}(\underline{y^T}\; \underline{y} - 2\underline{a}^{T}\underline{\underline{H}}^T\;\underline{y} + \underline{a}^T\;\underline{\underline{H}}^T\; \underline{\underline{H}}\;\underline{a})$$
\noindent Since,
$$(\underline{y}^T \; \underline{\underline{H}}\; \underline{a})^T = \underline{a}^T \; \underline{\underline{H}}^T\; \underline{y}$$
\noindent Therefore,
$$\frac{\partial q(a)}{\partial a_i} = 0 - 2 \; \underline{e_i}^T\; \underline{\underline{H}}^T \; \underline{y}+ \underline{e_i}^{T} \;\underline{\underline{H}}^T\;\underline{\underline{H}}\; \underline{a} + \underline{a}^T\;\underline{\underline{H}}^T\; \underline{\underline{H}}\;\underline{e_i} $$
\noindent where $e_i$ is a derivative of vector $a$ with respect to $i.$ The vector $e_i$ will be an identity matrix as the coefficient of $i$'s will have derivative as 1.
\noindent Then,
$$\frac{\partial q(a)}{\partial a_i} = -2 \; \underline{e_i}^T\; \underline{\underline{H}}^T \; \underline{y} + 2 \underline{e_i}^{T} \;\underline{\underline{H}}^T\;\underline{\underline{H}}\; \underline{a} $$
$$0 = -2 \; \underline{e_i}^T\; \underline{\underline{H}}^T \; \underline{y} + 2 \underline{e_i}^{T} \;\underline{\underline{H}}^T\;\underline{\underline{H}}\; \underline{a} $$
$$ \; \underline{e_i}^T\; \underline{\underline{H}}^T \; \underline{y} =  \underline{e_i}^{T} \;\underline{\underline{H}}^T\;\underline{\underline{H}}\; \underline{a} $$
\noindent If we take the individual terms, i.e., $i=0,1,2....p$, we can express in matrix domain and we get,
$$  \begin{bmatrix}
{e_1}^T\\
{e_2}^T\\
.\\
.\\
.\\
{e_p}^T\\
\end{bmatrix} 
 \underline{\underline{H}}^T\;\underline{\underline{H}}\; \underline{a} 
 = 
 \begin{bmatrix}
{e_1}^T\\
{e_2}^T\\
.\\
.\\
.\\
{e_p}^T\\ \end{bmatrix} \underline{\underline{H}}^T \; \underline{y}   $$
\noindent The two column matrices are identity matrices. Therefore,
$$ \underline{\underline{H}}^T\;\underline{\underline{H}}\; \underline{a} = \underline{\underline{H}}^T \; \underline{y}  $$
\noindent Now since $n \geq p+1$ we get,
$$\underline{\hat{a}} = (\underline{\underline{H^T}} \cdot \underline{\underline{H}})^{-1}\underline{\underline{H}}^T\;\underline{y}$$
\noindent We can get to know the coefficient of the polynomial through the above equation. Thus the least square error is given by,
$$q(\underline{\hat{a}}) = (\underline{y} - \underline{\underline{H}} \cdot \underline{\hat{a}})^T (\underline{y} - \underline{\underline{H}} \cdot \underline{\hat{a}}) $$
$$q(\underline{\hat{a}}) = (\underline{y}-\underline{\underline{H}} \cdot (\underline{\underline{H}}^T \cdot \underline{\underline{H}})^{-1}\underline{\underline{H}}^T \cdot \underline{y})^T
(\underline{y} - \underline{\underline{H}} \cdot (\underline{\underline{H}}^T\cdot \underline{\underline{H}})^{-1}\underline{\underline{H}}^T\cdot \underline{y}) $$
$$q(\underline{\hat{a}}) = (\underline{y} - \underline{\underline{P}}\cdot \underline{y})^T(\underline{y} - \underline{\underline{P}}\cdot \underline{y})$$
\noindent where $P$ is a projection matrix having idempotent property.
$$\underline{\underline{P}}\cdot\underline{\underline{P}} = \underline{\underline{P}}$$
$$\underline{\underline{P^\bot}}\cdot\underline{\underline{P^\bot}} = \underline{\underline{P^\bot}}$$
\noindent where $\underline{\underline{P}}^\bot = (\underline{\underline{I}}-\underline{\underline{P}})$
\noindent Therefore we get the equation as follows,
$$q(\hat{\underline{a}}) = ( (\underline{\underline{I}} - \underline{\underline{P}})\underline{y})^T( (\underline{\underline{I}}-\underline{\underline{P}})\underline{y}) $$ 
$$q(\hat{\underline{a}}) = \underline{y}^T(\underline{\underline{I}} -\underline{\underline{P}})^T(\underline{\underline{I}}-\underline{\underline{P}})\underline{y}$$
$$q(\hat{\underline{a}}) = \underline{y}^T(\underline{\underline{I}}-\underline{\underline{P}})\underline{y}$$
\noindent Therefore,
$$q(\hat{\underline{a}}) = \underline{y}^T\underline{\underline{P}}^\bot \underline{y}$$

\noindent \textbf{Estimate for variance of noise:}\\
\noindent SInce $Z_i \sim N(o, {\sigma_Z}^2),$ estimate of variance, ${\sigma_Z}^2$ is,
$$ {\sigma_Z}^2 = \frac{1}{N-1} \sum_{i=1}^{N}(Z_i - \mu_Z)^2  $$
\noindent Our signal is  $Y_i \sim N( a_0 + a_1 x_i + a_2 {x_i}^2 + ...+ a_p {x_i}^p, {\sigma_Z}^2) $
\noindent Therefore, 
$$ {\sigma_Z}^2 = \frac{1}{N-(P+1)} \sum_{i=1}^{N}(y_i - ( \hat{a}_0 + \hat{a}_1 x_i + \hat{a}_2 {x_i}^2 + ...+ \hat{a}_p {x_i}^p))^2 $$
\noindent The term $ \frac{1}{N-(P+1)}  $ is used because we have the same degree as the number of coefficient causing an unbiased estimator.
$$ {\sigma_Z}^2 = \frac{1}{N-(P+1)}(\underline{y} - \underline{\underline{H}} \cdot \underline{\hat{a}})^T (\underline{y} - \underline{\underline{H}} \cdot \underline{\hat{a}}) $$
$$ {\sigma_Z}^2 = \frac{1}{N-(P+1)}(\underline{y}-\underline{\underline{H}} \cdot (\underline{\underline{H}}^T \cdot \underline{\underline{H}})^{-1}\underline{\underline{H}}^T \cdot \underline{y})^T(\underline{y} -\underline{\underline{H}} \cdot (\underline{\underline{H}}^T\cdot \underline{\underline{H}})^{-1}\underline{\underline{H}}^T\cdot \underline{y}) $$
\noindent Substituting the value of $\underline{\hat{a}}$ and $P$ we get,
$$ {\sigma_Z}^2 = \frac{1}{N-(P+1)}(\underline{y} - \underline{\underline{P}} \;\underline{y})^T \; (\underline{y} - \underline{\underline{P}} \; \underline{y}) $$
$$ {\sigma_Z}^2 = \frac{1}{N-(P+1)}((I - \underline{\underline{P}}) \;\underline{y})^T \; ((I - \underline{\underline{P}}) \; \underline{y} $$
\noindent where $ (I - P) = \underline{\underline{P}}^\bot $ therefore,
$$ {\sigma_Z}^2 = \frac{1}{N-(P+1)}( \underline{\underline{P}}^\bot \;\underline{y})^T \; (\underline{\underline{P}}^\bot \; \underline{y}) $$
$$ {\sigma_Z}^2 = \frac{1}{N-(P+1)} \; \underline{y}^T \; \underline{\underline{P}}^\bot \; \underline{\underline{P}}^\bot \; \underline{y} $$
\noindent Therefore,
$$ {\sigma_Z}^2 = \frac{1}{N-(P+1)} \; \underline{y}^T \; \underline{\underline{P}}^\bot  \; \underline{y} $$

%%%************************************* section 2 ***********%%%%%%%%


\section{Power model and exponential model }
\noindent \textbf{Task:} The power model is given by
$$ Y_i = a \cdot {x_i}^b \cdot Z_i  $$
\noindent Linearise the model assuming ($a > 0, x_i > 0$) and $Z_i > 0.$ Why are the measuring errors assumed to be multiplicative? Estimate the parameters $a$ and $b$ as well as the variance of the measurement error after linearization. How large has to be the number $N$ of observations at least? The exponential model is defined by
$$  Y_i = a \cdot b^{x_i} \cdot Z_i   $$
\noindent Carry out all the tasks mentioned above also for this model.\\
\noindent \textbf{Solution:}\\
The power model is given by,
$$ Y_i = a \cdot {x_i}^b \cdot Z_i  $$
\noindent To get the value of the coefficients, we can define $q(a,b)$ where,
$$ q(a,b) = \sum_{i=1}^{N} (Y_i - a\; {x_i}^b)^2 $$
\noindent This equation leads to a non - linear solution because of the power $b.$ Therefore to get the iterative solution, we can linearize the power model by taking logarithms on both sides of the equation. Therefore,
$$\text{log}Y_i = \text{log}\;a + b\; \text{log}\; x_i + \text{log}\; Z_i$$
\noindent Hence, we get a linear model of the form,
$$\overset{\sim}{Y_i} = \overset{\sim}{a} + b\overset{\sim}{x_i} + \overset{\sim}{Z_i},\;\;\;\; i =1,2,...,N$$
\noindent where, $ \text{log}\; Y_i = \overset{\sim}{Y_i},\; \text{log}\; x_i = \overset{\sim}{x_i},\;\text{log}\; a_i = \overset{\sim}{a_i}, \;\text{and} \; \; \text{log}\; Z_i = \overset{\sim}{Z_i}$ are all positive.

\noindent The measuring errors are assumed to be multiplicative so that after linearization they become additive. And that assumption can be made because a bias in the estimation of the parameters a and b result in a multiplicative part in the error of the measurement dependent on $x_i$. In matrix notation, we can represent as,
\[
\begin{bmatrix}
\overset{\sim}{Y_1}\\
\overset{\sim}{Y_2}\\
.\\
.\\
.\\
\overset{\sim}{Y_N}\\
\end{bmatrix}
=
\begin{bmatrix}
1 & \overset{\sim}{x_1}\\
1 & \overset{\sim}{x_2}\\
. & .\\
. & .\\
. & .\\
1 & \overset{\sim}{x_N}
\end{bmatrix}
\begin{bmatrix}
\overset{\sim}{a}\\
b\\
\end{bmatrix}
+\begin{bmatrix}
\overset{\sim}{Z_1}\\
\overset{\sim}{Z_2}\\
.\\
.\\
.\\
\overset{\sim}{Z_N}
\end{bmatrix}
\]
\noindent The matrix can be written as $\underline{\overset{\sim}{Y}} = \underline{\underline{H}}\cdot\underline{\overset{\sim}{a}} + \underline{\overset{\sim}{Z}}$. This is the same linear model as in 1.1 and we can apply the concluded results.  We can determine the estimates for the parameters $a, b$ from the following equations,
$$\hat{\underline{\overset{\sim}{a}}} = (  \overset{\sim}{\underline{\underline{H}}^T}   \cdot \overset{\sim}{\underline{\underline{H}}})^{-1} \cdot  \overset{\sim}{\underline{\underline{H}}^T} \cdot \underline{\overset{\sim}{y}} $$
\noindent Thus we have,
$$ q(\overset{\sim}{\underline{a}}) = (\overset{\sim}{y} - \underline{\underline{\overset{\sim}{H}}}\; \underline{\overset{\sim}{a}})^T \cdot (\overset{\sim}{y} - \underline{\underline{\overset{\sim}{H}}}\; \underline{\overset{\sim}{a}})$$
\noindent The variance of the measurement error after linearization is given by,
$$\hat{\sigma_z^2} = \frac{\overset{\sim}{\underline{y}^T} \cdot \underline{\underline{p}}^{\bot}\cdot \underline{y}}{(N- (p+1))}$$
\noindent In this model, $(p + 1)= 2$ number of observations as two parameters are unknown, we need to determine two equations, i.e., $N > P+1.$
$$\hat{\sigma_z^2} =\frac{\overset{\sim}{\underline{y}^T} \cdot \underline{\underline{p}}^{\bot}\cdot \underline{y}}{(N - 2)}$$
where,
$$\underline{\underline{p}}^{\bot} = I -  \overset{\sim}{\underline{\underline{H}}} (\overset{\sim}{\underline{\underline{H}}^T}   \cdot \overset{\sim}{\underline{\underline{H}}})^{-1}  \cdot  \overset{\sim}{\underline{\underline{H}}^T} $$
\noindent Thus we have an accurate realization.\\


\noindent The exponential model is given as,
$$  Y_i = a \cdot b^{x_i} \cdot Z_i   $$
\noindent By taking the logarithms on both sides we get,
$$\text{log}\;Y_i = \text{log} \;a + x_i \text{log} \;b + \text{log} \;Z_i $$
\noindent Let $ \text{log}\; Y_i = \overset{\sim}{Y_i},\; \text{log}\; x_i = \overset{\sim}{x_i},\;\text{log}\; a_i = \overset{\sim}{a_i}, \;\text{and} \; \; \text{log}\; Z_i = \overset{\sim}{Z_i}$ 
\noindent Then we can write the equation in matrix domain and we get,
\[
\begin{bmatrix}
\overset{\sim}{Y_1}\\
\overset{\sim}{Y_2}\\
.\\
.\\
.\\
\overset{\sim}{Y_N}\\
\end{bmatrix}
=
\begin{bmatrix}
\text{log}\;Y_1 \\
\text{log}\;Y_2\\
.\\
.\\
.\\
\text{log}\;Y_N\\
\end{bmatrix}
=
\begin{bmatrix}
1 & {x_1}\\
1 & {x_2}\\
. & .\\
. & .\\
. & .\\
1 & {x_N}
\end{bmatrix}
\begin{bmatrix}
\overset{\sim}{a}\\
\overset{\sim}{b}\\
\end{bmatrix}
+\begin{bmatrix}
\overset{\sim}{Z_1}\\
\overset{\sim}{Z_2}\\
.\\
.\\
.\\
\overset{\sim}{Z_N}
\end{bmatrix}
\]
\noindent The matrix can be written as $\underline{\overset{\sim}{y}} = \underline{\underline{H}} \cdot \underline{\overset{\sim}{a}} + \underline{\overset{\sim}{Z}}$.
\noindent This is the same model as in section 1.1. Therefore we can determine the coefficients in the same way as we did in the previous case. Thus the variance of the measurement error after linearization is given by,
$$\hat{\sigma_z^2} =\frac{\overset{\sim}{\underline{y}^T} \cdot \underline{\underline{p}}^{\bot}\cdot \underline{\overset{\sim}{y}}}{(N - 2)}$$
where,
$$\underline{\underline{p}}^{\bot} = I -  \overset{\sim}{\underline{\underline{H}}} (\overset{\sim}{\underline{\underline{H}}^T}   \cdot \overset{\sim}{\underline{\underline{H}}})^{-1}  \cdot  \overset{\sim}{\underline{\underline{H}}^T} $$
\noindent The number of observations should also be at least 2 in order to determine the parameter. Thus we have an accurate realization.\\

%%%%%%%%*****section 3 *******%%%%%%%%%%

\section{Sine model }
\noindent \textbf{Task:} The amplitude $a$ and phase $b$ of the sine model
$$ Y_i = a \; \text{sin}(x_i + b) + Z_i  $$
\noindent are unknown. Linearize the sine model. Estimate the parameters $a$ and $b$ as well as the variance of the measurement error after linearization. How large has to be the number $N$ of observations at least?\\
\noindent \textbf{Solution:}\\
\noindent In this we assume that the frequency of the given sinusoidal signal is known and we need to determine the amplitude and phase of the sine signal, where signal is affected by noise. The sine model is given as,
$$ Y_i = a \; \text{sin}(x_i + b) + Z_i  $$
\noindent Now we define, 
$$q(a,b) = \sum_{i=1}^{N}(y_i - a\;\text{sin}(x_i + b))^2$$
\noindent In-order to linearize the given model we use, 
$$\text{sin}(x+y) = \text{sin}\;x\; \text{cos}\;y + \text{sin}\;y\; \text{cos}\; x$$
\noindent Therefore,
$$y_i = a\;\text{sin}\;x_i\;\text{cos}\;b + a\;\text{cos}\;x_i\;\text{sin}\;b + z_i$$
$$y_i = a\;\text{cos}\;b\;\text{sin}\;x_i + a\;\text{sin}\;b\;\text{cos}\;x_i +z_i$$
Let $a\;\text{cos}\;b = A$ and $a\;\text{sin}\;b = B$. Then for $ i=1,.....,N,$ the matrix notation is given by,
\[
\begin{bmatrix}
{y_1}\\
{y_2}\\
.\\
.\\
.\\
{y_N}\\
\end{bmatrix}
=
\begin{bmatrix}
\text{sin}\;x_1& \text{cos}\;x_1\\
\text{sin}\;x_2& \text{cos}\;x_2\\
. & .\\
. & .\\
. & .\\
\text{sin}\;x_N& \text{cos}\;x_N\\
\end{bmatrix}
\begin{bmatrix}
A\\
B\\
\end{bmatrix}
=\begin{bmatrix}
{z_1}\\
.\\
.\\
.\\
.\\
{z_N}
\end{bmatrix}
\]
\noindent The matrix can be written as $\underline{\overset{\sim}{y}} = \underline{\underline{H}} \cdot \underline{\overset{\sim}{a}} + \underline{\overset{\sim}{Z}}$. For the sine model, the measurement will be,
$$ q(A,B) = (\overset{\sim}{y} - \underline{\underline{\overset{\sim}{H}}}\; \underline{\overset{\sim}{a}})^T \cdot (\overset{\sim}{y} - \underline{\underline{\overset{\sim}{H}}}\; \underline{\overset{\sim}{a}})$$
\noindent The criteria to get the coefficients $A$ and $B$ is,
\[
{\underline{\hat{a}}} =
\begin{bmatrix}
\hat{A} \\
\hat{B} \\
\end{bmatrix}
=
(  \overset{\sim}{\underline{\underline{H}}^T}   \cdot \overset{\sim}{\underline{\underline{H}}})^{-1} \cdot  \overset{\sim}{\underline{\underline{H}}^T} \cdot \underline{\overset{\sim}{y}}
\]
\noindent Hence solving the equation we have,
$$ \hat{A} = \hat{a}\;\text{cos}\;\hat{b}\\$$
$$ \hat{B} = \hat{a}\;\text{sin}\;\hat{b}\\$$
\noindent Therefore we get the parameters as,
$$\hat{b} = \text{arc}\;\text{tan}\frac{\hat{B}}{\hat{A}}$$
$$\hat{a} = \frac{\hat{A}}{\text{cos}(\text{arc}\;\text{tan}\frac{\hat{B}}{\hat{A}})}$$
\noindent Also the ampliture ${\hat{A}}^2 + {\hat{B}}^2 = {\hat{a}}^2$. Now to get the variance with number of observations as (p+1),
$$\hat{\sigma_z^2} = \frac{\underline{y}^T\cdot \underline{\underline{p}}^{\bot}\cdot \underline{y}}{(N- (p+1))}$$
\noindent Here also we have 2 equations. Hence, the number of observations will be 2. Also from section 1.1,
$$\underline{\underline{p}}^{\bot} = \underline{\underline{I}} - \underline{\underline{H}}(\underline{\underline{H}}^T.\underline{\underline{H}})^{-1}\underline{\underline{H}}^T$$
\noindent Therefore,
$$\hat{\sigma_z^2} = \frac{\underline{y}^T\cdot \underline{\underline{p}}^{\bot}\cdot \underline{y}}{(N-2)}$$



